{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import cupy as cp\n",
    "import cuml as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>MIDCount</th>\n",
       "      <th>ExonCount</th>\n",
       "      <th>CellID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tmlhe</td>\n",
       "      <td>10618</td>\n",
       "      <td>12270</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tmlhe</td>\n",
       "      <td>8222</td>\n",
       "      <td>15645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tmlhe</td>\n",
       "      <td>8917</td>\n",
       "      <td>15028</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tmlhe</td>\n",
       "      <td>9046</td>\n",
       "      <td>14970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tmlhe</td>\n",
       "      <td>10599</td>\n",
       "      <td>14981</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geneID      x      y  MIDCount  ExonCount  CellID\n",
       "0  Tmlhe  10618  12270         1          1    6954\n",
       "1  Tmlhe   8222  15645         1          1   47144\n",
       "2  Tmlhe   8917  15028         1          1   39597\n",
       "3  Tmlhe   9046  14970         1          1   38748\n",
       "4  Tmlhe  10599  14981         1          1   39019"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CellID'] = data['CellID'].astype(np.uint16)\n",
    "data['ExonCount'] = data['ExonCount'].astype(np.uint8)\n",
    "data['MIDCount'] = data['MIDCount'].astype(np.uint8)\n",
    "data['x'] = data['x'].astype(np.uint16)\n",
    "data['y'] = data['y'].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cells = data['CellID'].unique()\n",
    "unique_genes = data['geneID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62725 entries, 6954 to 22645\n",
      "Columns: 20753 entries, Tmlhe to Gm42418\n",
      "dtypes: uint8(20753)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "def midcount_filling() -> int:\n",
    "    gpu_matrix = cp.zeros((len(unique_cells), len(unique_genes)), dtype=cp.uint8)\n",
    "    cell_indices = pd.factorize(data['CellID'])[0] \n",
    "    gene_indices = pd.factorize(data['geneID'])[0]\n",
    "\n",
    "    gpu_cell_indices = cp.array(cell_indices, dtype=cp.uint16)\n",
    "    gpu_gene_indices = cp.array(gene_indices, dtype=cp.uint16)\n",
    "    gpu_midcount = cp.array(data['MIDCount'].values, dtype=cp.uint8)\n",
    "\n",
    "    gpu_matrix[gpu_cell_indices, gpu_gene_indices] = gpu_midcount\n",
    "\n",
    "    tmp_matrix = cp.asnumpy(gpu_matrix)\n",
    "\n",
    "    unloged_matrix = pd.DataFrame(tmp_matrix, index=unique_cells, columns=unique_genes, dtype=np.uint8)\n",
    "    unloged_matrix.to_csv('unloged_matrix.csv', index=False)\n",
    "\n",
    "    unloged_matrix.info()\n",
    "    return unloged_matrix.last_valid_index()\n",
    "\n",
    "unloged_matrix_number_of_rows = midcount_filling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22645"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unloged_matrix_number_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading first part\n",
      "first part is loaded\n",
      "starting loging first part\n",
      "first part is loged\n",
      "loading second part\n",
      "second part is loaded\n",
      "starting loging second part\n",
      "second part is loged\n"
     ]
    }
   ],
   "source": [
    "half_rows = unloged_matrix_number_of_rows // 2\n",
    "second_half_rows = unloged_matrix_number_of_rows - half_rows\n",
    "\n",
    "def loging_matrix(data_chunk, mode='w') -> None:\n",
    "    gpu_matrix = cp.array(data_chunk.values, dtype=cp.float32)\n",
    "\n",
    "    gpu_matrix[gpu_matrix == 0] = -1\n",
    "    gpu_matrix = cp.where(gpu_matrix == -1, -1, cp.log(gpu_matrix))\n",
    "\n",
    "    tmp_matrix = cp.asnumpy(gpu_matrix)\n",
    "\n",
    "    loged_matrix = pd.DataFrame(tmp_matrix, index=data_chunk.index, columns=data_chunk.columns, dtype=np.float32)\n",
    "    loged_matrix.to_csv('loged_matrix.csv', mode=mode, header=(mode == 'w'), index=False)\n",
    "\n",
    "def first_part() -> None:\n",
    "    print(f'loading first part')\n",
    "    first_half = pd.read_csv('unloged_matrix.csv', nrows=half_rows)\n",
    "    print(f'first part is loaded')\n",
    "    print(f'starting loging first part')\n",
    "    loging_matrix(first_half, mode='w')\n",
    "    print(f'first part is loged')\n",
    "\n",
    "def second_part() -> None:\n",
    "    print(f'loading second part')\n",
    "    second_half = pd.read_csv('unloged_matrix.csv', skiprows=half_rows, nrows=second_half_rows)\n",
    "    print(f'second part is loaded')\n",
    "    print(f'starting loging second part')\n",
    "    loging_matrix(second_half, mode='a')\n",
    "    print(f'second part is loged') \n",
    "\n",
    "first_part()\n",
    "second_part()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HDBSCAN in module cuml.cluster.hdbscan.hdbscan:\n",
      "\n",
      "class HDBSCAN(cuml.internals.base.UniversalBase, cuml.internals.mixins.ClusterMixin, cuml.internals.mixins.CMajorInputTagMixin)\n",
      " |  HDBSCAN(*, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=2, cluster_selection_method='eom', allow_single_cluster=False, gen_min_span_tree=False, handle=None, verbose=False, connectivity='knn', output_type=None, prediction_data=False)\n",
      " |  \n",
      " |  HDBSCAN Clustering\n",
      " |  \n",
      " |  Recursively merges the pair of clusters that minimally increases a\n",
      " |  given linkage distance.\n",
      " |  \n",
      " |  Note that while the algorithm is generally deterministic and should\n",
      " |  provide matching results between RAPIDS and the Scikit-learn Contrib\n",
      " |  versions, the construction of the k-nearest neighbors graph and\n",
      " |  minimum spanning tree can introduce differences between the two\n",
      " |  algorithms, especially when several nearest neighbors around a\n",
      " |  point might have the same distance. While the differences in\n",
      " |  the minimum spanning trees alone might be subtle, they can\n",
      " |  (and often will) lead to some points being assigned different\n",
      " |  cluster labels between the two implementations.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  handle : cuml.Handle\n",
      " |      Specifies the cuml.handle that holds internal CUDA state for\n",
      " |      computations in this model. Most importantly, this specifies the CUDA\n",
      " |      stream that will be used for the model's computations, so users can\n",
      " |      run different models concurrently in different streams by creating\n",
      " |      handles in several streams.\n",
      " |      If it is None, a new one is created.\n",
      " |  \n",
      " |  alpha : float, optional (default=1.0)\n",
      " |      A distance scaling parameter as used in robust single linkage.\n",
      " |  \n",
      " |  verbose : int or boolean, default=False\n",
      " |      Sets logging level. It must be one of `cuml.common.logger.level_*`.\n",
      " |      See :ref:`verbosity-levels` for more info.\n",
      " |  \n",
      " |  min_cluster_size : int, optional (default = 5)\n",
      " |      The minimum number of samples in a group for that group to be\n",
      " |      considered a cluster; groupings smaller than this size will be left\n",
      " |      as noise.\n",
      " |  \n",
      " |  min_samples : int, optional (default=None)\n",
      " |      The number of samples in a neighborhood for a point\n",
      " |      to be considered as a core point. This includes the point itself.\n",
      " |      If 'None', it defaults to the min_cluster_size.\n",
      " |  \n",
      " |  cluster_selection_epsilon : float, optional (default=0.0)\n",
      " |      A distance threshold. Clusters below this value will be merged.\n",
      " |      Note that this should not be used\n",
      " |      if we want to predict the cluster labels for new points in future\n",
      " |      (e.g. using approximate_predict), as the approximate_predict function\n",
      " |      is not aware of this argument.\n",
      " |  \n",
      " |  max_cluster_size : int, optional (default=0)\n",
      " |      A limit to the size of clusters returned by the eom algorithm.\n",
      " |      Has no effect when using leaf clustering (where clusters are\n",
      " |      usually small regardless) and can also be overridden in rare\n",
      " |      cases by a high value for cluster_selection_epsilon. Note that\n",
      " |      this should not be used if we want to predict the cluster labels\n",
      " |      for new points in future (e.g. using approximate_predict), as\n",
      " |      the approximate_predict function is not aware of this argument.\n",
      " |  \n",
      " |  metric : string or callable, optional (default='euclidean')\n",
      " |      The metric to use when calculating distance between instances in a\n",
      " |      feature array. If metric is a string or callable, it must be one of\n",
      " |      the options allowed by metrics.pairwise.pairwise_distances for its\n",
      " |      metric parameter.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square.\n",
      " |  \n",
      " |  p : int, optional (default=2)\n",
      " |      p value to use if using the minkowski metric.\n",
      " |  \n",
      " |  cluster_selection_method : string, optional (default='eom')\n",
      " |      The method used to select clusters from the condensed tree. The\n",
      " |      standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n",
      " |      to find the most persistent clusters. Alternatively you can instead\n",
      " |      select the clusters at the leaves of the tree -- this provides the\n",
      " |      most fine grained and homogeneous clusters. Options are:\n",
      " |  \n",
      " |          * ``eom``\n",
      " |          * ``leaf``\n",
      " |  \n",
      " |  allow_single_cluster : bool, optional (default=False)\n",
      " |      By default HDBSCAN* will not produce a single cluster, setting this\n",
      " |      to True will override this and allow single cluster results in\n",
      " |      the case that you feel this is a valid result for your dataset.\n",
      " |  \n",
      " |  gen_min_span_tree : bool, optional (default=False)\n",
      " |      Whether to populate the `minimum_spanning_tree_` member for\n",
      " |      utilizing plotting tools. This requires the `hdbscan` CPU Python\n",
      " |      package to be installed.\n",
      " |  \n",
      " |  gen_condensed_tree : bool, optional (default=False)\n",
      " |      Whether to populate the `condensed_tree_` member for\n",
      " |      utilizing plotting tools. This requires the `hdbscan` CPU\n",
      " |      Python package to be installed.\n",
      " |  \n",
      " |  gen_single_linkage_tree_ : bool, optional (default=False)\n",
      " |      Whether to populate the `single_linkage_tree_` member for\n",
      " |      utilizing plotting tools. This requires the `hdbscan` CPU\n",
      " |      Python package t be installed.\n",
      " |  \n",
      " |  output_type : {'input', 'array', 'dataframe', 'series', 'df_obj',         'numba', 'cupy', 'numpy', 'cudf', 'pandas'}, default=None\n",
      " |      Return results and set estimator attributes to the indicated output\n",
      " |      type. If None, the output type set at the module level\n",
      " |      (`cuml.global_settings.output_type`) will be used. See\n",
      " |      :ref:`output-data-type-configuration` for more info.\n",
      " |  \n",
      " |  prediction_data : bool, optional (default=False)\n",
      " |      Whether to generate extra cached data for predicting labels or\n",
      " |      membership vectors few new unseen points later. If you wish to\n",
      " |      persist the clustering object for later re-use you probably want\n",
      " |      to set this to True.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  labels_ : ndarray, shape (n_samples, )\n",
      " |      Cluster labels for each point in the dataset given to fit().\n",
      " |      Noisy samples are given the label -1.\n",
      " |  \n",
      " |  probabilities_ : ndarray, shape (n_samples, )\n",
      " |      The strength with which each sample is a member of its assigned\n",
      " |      cluster. Noise points have probability zero; points in clusters\n",
      " |      have values assigned proportional to the degree that they\n",
      " |      persist as part of the cluster.\n",
      " |  \n",
      " |  cluster_persistence_ : ndarray, shape (n_clusters, )\n",
      " |      A score of how persistent each cluster is. A score of 1.0 represents\n",
      " |      a perfectly stable cluster that persists over all distance scales,\n",
      " |      while a score of 0.0 represents a perfectly ephemeral cluster. These\n",
      " |      scores can be used to gauge the relative coherence of the\n",
      " |      clusters output by the algorithm.\n",
      " |  \n",
      " |  condensed_tree_ : CondensedTree object\n",
      " |      The condensed tree produced by HDBSCAN. The object has methods\n",
      " |      for converting to pandas, networkx, and plotting.\n",
      " |  \n",
      " |  single_linkage_tree_ : SingleLinkageTree object\n",
      " |      The single linkage tree produced by HDBSCAN. The object has methods\n",
      " |      for converting to pandas, networkx, and plotting.\n",
      " |  \n",
      " |  minimum_spanning_tree_ : MinimumSpanningTree object\n",
      " |      The minimum spanning tree of the mutual reachability graph generated\n",
      " |      by HDBSCAN. Note that this is not generated by default and will only\n",
      " |      be available if `gen_min_span_tree` was set to True on object creation.\n",
      " |      Even then in some optimized cases a tree may not be generated.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HDBSCAN\n",
      " |      cuml.internals.base.UniversalBase\n",
      " |      cuml.internals.base.Base\n",
      " |      cuml.internals.mixins.TagsMixin\n",
      " |      cuml.internals.mixins.ClusterMixin\n",
      " |      cuml.internals.mixins.CMajorInputTagMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __dealloc__(self)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__ = inner_f(self, *, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=2, cluster_selection_method='eom', allow_single_cluster=False, gen_min_span_tree=False, handle=None, verbose=False, connectivity='knn', output_type=None, prediction_data=False) from cuml.internals.api_decorators._deprecate_pos_args.__call__.<locals>\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  build_minimum_spanning_tree(self, X)\n",
      " |  \n",
      " |  fit(self, X, y=None, convert_dtype=True) -> \"'HDBSCAN'\"\n",
      " |              Fit HDBSCAN model from features.\n",
      " |              \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      \n",
      " |      X : array-like (device or host) shape = (n_samples, n_features)\n",
      " |          Dense matrix. If datatype is other than floats or doubles,\n",
      " |          then the data will be converted to float which increases memory\n",
      " |          utilization. Set the parameter convert_dtype to False to avoid \n",
      " |          this, then the method will throw an error instead.  \n",
      " |          Acceptable formats: CUDA array interface compliant objects like\n",
      " |          CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas\n",
      " |          DataFrame/Series.\n",
      " |      \n",
      " |      y : array-like (device or host) shape = (n_samples, 1)\n",
      " |          Dense matrix. If datatype is other than floats or doubles,\n",
      " |          then the data will be converted to float which increases memory\n",
      " |          utilization. Set the parameter convert_dtype to False to avoid \n",
      " |          this, then the method will throw an error instead.  \n",
      " |          Acceptable formats: CUDA array interface compliant objects like\n",
      " |          CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas\n",
      " |          DataFrame/Series.\n",
      " |      \n",
      " |      convert_dtype : bool, optional (default = True)\n",
      " |          When set to True, the train method will, when necessary, convert\n",
      " |          y to be the same data type as X if they differ. This\n",
      " |          will increase memory used for the method.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None) -> 'CumlArray'\n",
      " |              Fit the HDBSCAN model from features and return\n",
      " |              cluster labels.\n",
      " |              \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      \n",
      " |      X : array-like (device or host) shape = (n_samples, n_features)\n",
      " |          Dense matrix. If datatype is other than floats or doubles,\n",
      " |          then the data will be converted to float which increases memory\n",
      " |          utilization. Set the parameter convert_dtype to False to avoid \n",
      " |          this, then the method will throw an error instead.  \n",
      " |          Acceptable formats: CUDA array interface compliant objects like\n",
      " |          CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas\n",
      " |          DataFrame/Series.\n",
      " |      \n",
      " |      y : array-like (device or host) shape = (n_samples, 1)\n",
      " |          Dense matrix. If datatype is other than floats or doubles,\n",
      " |          then the data will be converted to float which increases memory\n",
      " |          utilization. Set the parameter convert_dtype to False to avoid \n",
      " |          this, then the method will throw an error instead.  \n",
      " |          Acceptable formats: CUDA array interface compliant objects like\n",
      " |          CuPy, cuDF DataFrame/Series, NumPy ndarray and Pandas\n",
      " |          DataFrame/Series.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preds : cuDF, CuPy or NumPy object depending on cuML's output type configuration, shape = (n_samples, 1)\n",
      " |          Cluster indexes\n",
      " |      \n",
      " |          For more information on how to configure cuML's output type,\n",
      " |          refer to: `Output Data Type Configuration`_.\n",
      " |  \n",
      " |  generate_prediction_data(self)\n",
      " |      Create data that caches intermediate results used for predicting\n",
      " |      the label of new/unseen points. This data is only useful if you\n",
      " |      are intending to use functions from hdbscan.prediction.\n",
      " |  \n",
      " |  get_attr_names(self)\n",
      " |  \n",
      " |  get_param_names(self)\n",
      " |      Returns a list of hyperparameter names owned by this class. It is\n",
      " |      expected that every child class overrides this method and appends its\n",
      " |      extra set of parameters that it in-turn owns. This is to simplify the\n",
      " |      implementation of `get_params` and `set_params` methods.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  children_\n",
      " |  \n",
      " |  cluster_persistence_\n",
      " |  \n",
      " |  condensed_tree_\n",
      " |  \n",
      " |  labels_\n",
      " |  \n",
      " |  lambdas_\n",
      " |  \n",
      " |  mst_dst_\n",
      " |  \n",
      " |  mst_src_\n",
      " |  \n",
      " |  mst_weights_\n",
      " |  \n",
      " |  outlier_scores_\n",
      " |  \n",
      " |  prediction_data_\n",
      " |  \n",
      " |  probabilities_\n",
      " |  \n",
      " |  single_linkage_tree_\n",
      " |  \n",
      " |  sizes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  children__order = 'K'\n",
      " |  \n",
      " |  cluster_persistence__order = 'K'\n",
      " |  \n",
      " |  labels__order = 'K'\n",
      " |  \n",
      " |  lambdas__order = 'K'\n",
      " |  \n",
      " |  mst_dst__order = 'K'\n",
      " |  \n",
      " |  mst_src__order = 'K'\n",
      " |  \n",
      " |  mst_weights__order = 'K'\n",
      " |  \n",
      " |  outlier_scores__order = 'K'\n",
      " |  \n",
      " |  probabilities__order = 'K'\n",
      " |  \n",
      " |  sizes__order = 'K'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from cuml.internals.base.UniversalBase:\n",
      " |  \n",
      " |  args_to_cpu(self, *args, **kwargs)\n",
      " |  \n",
      " |  build_cpu_model(self)\n",
      " |  \n",
      " |  cpu_to_gpu(self)\n",
      " |  \n",
      " |  dispatch_func(self, func_name, gpu_func, *args, **kwargs)\n",
      " |      This function will dispatch calls to training and inference according\n",
      " |      to the global configuration. It should work for all estimators\n",
      " |      sufficiently close the scikit-learn implementation as it uses\n",
      " |      it for training and inferences on host.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func_name : string\n",
      " |          name of the function to be dispatched\n",
      " |      gpu_func : function\n",
      " |          original cuML function\n",
      " |      args : arguments\n",
      " |          arguments to be passed to the function for the call\n",
      " |      kwargs : keyword arguments\n",
      " |          keyword arguments to be passed to the function for the call\n",
      " |  \n",
      " |  gpu_to_cpu(self)\n",
      " |  \n",
      " |  import_cpu_model(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from cuml.internals.base.Base:\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      " |      Redirects to `solver_model` if the attribute exists.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Pretty prints the arguments of a class using Scikit-learn standard :)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Returns a dict of all params owned by this class. If the child class\n",
      " |      has appropriately overridden the `get_param_names` method and does not\n",
      " |      need anything other than what is there in this method, then it doesn't\n",
      " |      have to override this method\n",
      " |  \n",
      " |  set_nvtx_annotations(self)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Accepts a dict of params and updates the corresponding ones owned by\n",
      " |      this class. If the child class has appropriately overridden the\n",
      " |      `get_param_names` method and does not need anything other than what is,\n",
      " |      there in this method, then it doesn't have to override this method\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from cuml.internals.mixins.TagsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load matrix\n",
      "Finished loading matrix\n",
      "Converting matrix to cp.array\n",
      "Finished conversion\n",
      "Fitting data\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /home/petar/anaconda3/envs/rapids-24.08/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     hdbscan\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished fitting data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m, in \u001b[0;36mclustering\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m hdbscan \u001b[38;5;241m=\u001b[39m HDBSCAN(min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mhdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished fitting data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/api_decorators.py:188\u001b[0m, in \u001b[0;36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     set_api_output_dtype(output_dtype)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[0;32m--> 188\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/api_decorators.py:393\u001b[0m, in \u001b[0;36menable_device_interop.<locals>.dispatch\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispatch_func\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    392\u001b[0m     func_name \u001b[38;5;241m=\u001b[39m gpu_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpu_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/api_decorators.py:190\u001b[0m, in \u001b[0;36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m         ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cm\u001b[38;5;241m.\u001b[39mprocess_return(ret)\n",
      "File \u001b[0;32mbase.pyx:687\u001b[0m, in \u001b[0;36mcuml.internals.base.UniversalBase.dispatch_func\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mhdbscan.pyx:762\u001b[0m, in \u001b[0;36mcuml.cluster.hdbscan.hdbscan.HDBSCAN.fit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/nvtx/nvtx.py:116\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 116\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/input_utils.py:412\u001b[0m, in \u001b[0;36minput_to_cuml_array\u001b[0;34m(X, order, deepcopy, check_dtype, convert_to_dtype, check_mem_type, convert_to_mem_type, safe_dtype_conversion, check_cols, check_rows, fail_on_order, force_contiguous)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;129m@nvtx_annotate\u001b[39m(\n\u001b[1;32m    325\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommon.input_utils.input_to_cuml_array\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    326\u001b[0m     category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m     force_contiguous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ):\n\u001b[1;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    Convert input X to CumlArray.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mCumlArray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_to_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_to_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_mem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_mem_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_to_mem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_to_mem_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_dtype_conversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_dtype_conversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfail_on_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_contiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_contiguous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         shape \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39m__cuda_array_interface__[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/memory_utils.py:87\u001b[0m, in \u001b[0;36mwith_cupy_rmm.<locals>.cupy_rmm_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GPU_ENABLED:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cupy_using_allocator(rmm_cupy_allocator):\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/nvtx/nvtx.py:116\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 116\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/array.py:1191\u001b[0m, in \u001b[0;36mCumlArray.from_input\u001b[0;34m(cls, X, order, deepcopy, check_dtype, convert_to_dtype, check_mem_type, convert_to_mem_type, safe_dtype_conversion, check_cols, check_rows, fail_on_order, force_contiguous)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         data \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mmem_type\u001b[38;5;241m.\u001b[39mxpy\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m   1188\u001b[0m             arr\u001b[38;5;241m.\u001b[39mto_output(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m), order\u001b[38;5;241m=\u001b[39morder\n\u001b[1;32m   1189\u001b[0m         )\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1191\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(data, index\u001b[38;5;241m=\u001b[39mindex)\n\u001b[1;32m   1197\u001b[0m n_rows \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/cupy/_creation/from_data.py:88\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, blocking)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masarray\u001b[39m(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts an object to array.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    This is equivalent to ``array(a, dtype, copy=False, order=order)``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2408\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2418\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2449\u001b[0m, in \u001b[0;36mcupy._core.core._array_from_cupy_ndarray\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:518\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:576\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:137\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:225\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:738\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.08/lib/python3.11/site-packages/rmm/allocators/cupy.py:37\u001b[0m, in \u001b[0;36mrmm_cupy_allocator\u001b[0;34m(nbytes)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo module named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcupy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m stream \u001b[38;5;241m=\u001b[39m Stream(obj\u001b[38;5;241m=\u001b[39mcupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_current_stream())\n\u001b[0;32m---> 37\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[43mlibrmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeviceBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m dev_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mptr \u001b[38;5;28;01melse\u001b[39;00m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mget_device_id()\n\u001b[1;32m     39\u001b[0m mem \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mUnownedMemory(\n\u001b[1;32m     40\u001b[0m     ptr\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39mptr, size\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39msize, owner\u001b[38;5;241m=\u001b[39mbuf, device_id\u001b[38;5;241m=\u001b[39mdev_id\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32mdevice_buffer.pyx:96\u001b[0m, in \u001b[0;36mrmm._lib.device_buffer.DeviceBuffer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /home/petar/anaconda3/envs/rapids-24.08/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "def clustering() -> None:\n",
    "    print(f'Starting to load matrix')\n",
    "    loged_matrix = pd.read_csv('loged_matrix.csv', index_col=0)\n",
    "    print(f'Finished loading matrix')\n",
    "    print(f'Converting matrix to cp.array')\n",
    "    data = cp.array(loged_matrix.values)\n",
    "    print(f'Finished conversion')\n",
    "\n",
    "    hdbscan = HDBSCAN(min_cluster_size=100)\n",
    "    print(f'Fitting data')\n",
    "    hdbscan.fit(data)\n",
    "    print(f'Finished fitting data')\n",
    "\n",
    "clustering()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
