{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.metrics import silhouette_score\n",
    "#from sklearn.decomposition import PCA\n",
    "#import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Changing dtype to reduce memory footprint, shifting X and Y coordinates to (0,0) and getting picture dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset.csv', index_col=0)\n",
    "\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()\n",
    "\n",
    "data['CellID'] = data['CellID'].astype(np.uint16)\n",
    "data['ExonCount'] = data['ExonCount'].astype(np.uint8)\n",
    "data['MIDCount'] = data['MIDCount'].astype(np.uint8)\n",
    "data['x'] = data['x'].astype(np.uint16)\n",
    "data['y'] = data['y'].astype(np.uint16)\n",
    "\n",
    "unique_cells = data['CellID'].unique()\n",
    "unique_genes = data['geneID'].unique()\n",
    "print(f'{len(unique_cells)} unique cells and {len(unique_genes)} unique genes')\n",
    "\n",
    "xmin, ymin = data['x'].min(), data['y'].min()\n",
    "data['x'], data['y'] = data['x'] - xmin, data['y'] - ymin\n",
    "xmax, ymax = data['x'].max(), data['y'].max()\n",
    "xmax += 1\n",
    "ymax += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating picture of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_picture() -> np.ndarray:\n",
    "    __data = data[['x', 'y']]\n",
    "    picture = np.zeros(shape=(xmax, ymax), dtype=np.uint8)\n",
    "\n",
    "    for row in range(__data.shape[0]):\n",
    "        x, y = __data.iloc[row]\n",
    "        picture[x][y] = 1\n",
    "\n",
    "    plt.figure(figsize=(xmax/100, ymax/100))\n",
    "    plt.imshow(picture, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    if not os.path.exists('cells.tiff'):\n",
    "        plt.imsave('./cells.tiff', picture, cmap='gray')\n",
    "\n",
    "    return picture\n",
    "\n",
    "picture = generate_picture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating new matrix cells/genes and filling it with MIDCount value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midcount_filling() -> int:\n",
    "    matrix = np.zeros((len(unique_cells), len(unique_genes)), dtype=np.uint8)\n",
    "    cell_indices = pd.factorize(data['CellID'])[0] \n",
    "    gene_indices = pd.factorize(data['geneID'])[0]\n",
    "\n",
    "    matrix_cell_indices = np.array(cell_indices, dtype=np.uint16)\n",
    "    matrix_gene_indices = np.array(gene_indices, dtype=np.uint16)\n",
    "    matrix_midcount = np.array(data['MIDCount'].values, dtype=np.uint8)\n",
    "\n",
    "    matrix[matrix_cell_indices, matrix_gene_indices] = matrix_midcount\n",
    "\n",
    "    unloged_matrix = pd.DataFrame(matrix, index=unique_cells, columns=unique_genes, dtype=np.uint8)\n",
    "    unloged_matrix.to_csv('unloged_matrix.csv')\n",
    "\n",
    "    unloged_matrix.info()\n",
    "    return unloged_matrix.last_valid_index()\n",
    "\n",
    "unloged_matrix_number_of_rows = midcount_filling()\n",
    "\n",
    "half_rows = unloged_matrix_number_of_rows // 2\n",
    "second_half_rows = unloged_matrix_number_of_rows - half_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Separated loader to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_name : str, option: str, dtype : str = None) -> any:\n",
    "    '''\n",
    "    Type of options:\\n\n",
    "    pddf -> returns loaded data as pd.DataFrame\\n\n",
    "    pddft -> returns loaded data as pd.DataFrame just transponed\\n\n",
    "    npnd -> returns loaded data as np.ndarray of dtype\\n\n",
    "    '''\n",
    "    data = pd.read_csv(file_name, index_col=0)\n",
    "\n",
    "    if option == 'pddf':\n",
    "        return data\n",
    "    if option == 'pddft':\n",
    "        return data.T\n",
    "    if option == 'npnd':\n",
    "        return np.array(data.values, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparing genes similarity for all cells, if 2 different genes have similarity above the threshold, second one will be marked for dropping.\n",
    "After first transformation number of genes may be drastically reduced, it depends on similarity percent that need's to be matched.<br>\n",
    "Point is to reduce number of features with minimal loss of meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 1\n",
    "\n",
    "def transformationR(similarity : float, file : str | pd.DataFrame) -> pd.DataFrame:\n",
    "    rows_to_drop : list = []\n",
    "    if isinstance(file, str):\n",
    "        transponed_data : pd.DataFrame = load_csv_data(file, option='pddft')\n",
    "    if isinstance(file, pd.DataFrame):\n",
    "        transponed_data = file.T\n",
    "\n",
    "    global cnt\n",
    "\n",
    "    print(cnt)\n",
    "    print(transponed_data.shape)\n",
    "    \n",
    "    #if cnt >= file.shape[0]:\n",
    "    #    reduced.to_csv('reduced_matrix.csv')\n",
    "    #    return transponed_data.T \n",
    "\n",
    "    row = transponed_data.index[cnt-1]\n",
    "\n",
    "    current_row = transponed_data.loc[row].values\n",
    "\n",
    "    for next_row in transponed_data.index[cnt:]: \n",
    "        second_row = transponed_data.loc[next_row].values\n",
    "\n",
    "        match = (current_row == second_row).sum() / second_row.__len__()\n",
    "        \n",
    "        if match >= similarity:\n",
    "            if match not in rows_to_drop:\n",
    "                rows_to_drop.append(next_row)\n",
    "    \n",
    "    transponed_data.drop(rows_to_drop, inplace=True)\n",
    "    cnt += 1\n",
    "\n",
    "    return transponed_data.T \n",
    "\n",
    "reduced = transformationR(0.999, 'unloged_matrix.csv')\n",
    "\n",
    "for i in range(2000):\n",
    "    reduced = transformationR(0.999, reduced)\n",
    "\n",
    "reduced.to_csv('reduced_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loging cells/genes matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loging_matrix() -> None:\n",
    "    data_chunk = load_csv_data('reduced_matrix.csv', 'pddf', None)\n",
    "    matrix = np.array(data_chunk.values, dtype=np.float32)\n",
    "\n",
    "    matrix[matrix == 0] = -1\n",
    "    matrix = np.where(matrix == -1, -1, np.log(matrix))\n",
    "\n",
    "    loged_matrix = pd.DataFrame(matrix, index=data_chunk.index, columns=data_chunk.columns, dtype=np.float32)\n",
    "    loged_matrix.to_csv('reduced_loged_matrix.csv')\n",
    "\n",
    "loging_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_(data : pd.DataFrame) -> any:\n",
    "    pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering() -> any:\n",
    "    X = load_csv_data('reduced_loged_matrix.csv', option='pddf')\n",
    "    model = Birch(n_clusters=14)\n",
    "    model.fit(X)\n",
    "    #joblib.dump(model, 'model.pkl')\n",
    "    \n",
    "    ##labels_df = pd.DataFrame(model.labels_)\n",
    "\n",
    "\n",
    "    score = silhouette_score(X, model.labels_)#\n",
    "    print(score)#    \n",
    "\n",
    "    return model.labels_\n",
    "\n",
    "labels = clustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Measuring silhouette score, values goes from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(labels : list) -> None:\n",
    "    data = load_csv_data('reduced_matrix.csv', option='pddf')\n",
    "    score = silhouette_score(data, labels)\n",
    "    print(score)\n",
    "\n",
    "score(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating new picture of clustered cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clustered_picture() -> None:\n",
    "    __data = data[['x', 'y', 'CellID']]\n",
    "    __labels = pd.DataFrame(labels, index=unique_cells)\n",
    "    picture = np.zeros(shape=(xmax, ymax), dtype=np.uint8)\n",
    "\n",
    "    for row in range(__data.shape[0]):\n",
    "        x, y, cellid = __data.iloc[row]\n",
    "        picture[x][y] = __labels.loc[cellid][0]\n",
    "\n",
    "    cmap = plt.get_cmap('tab20', 20) ###\n",
    "    tmp = list(cmap.colors)\n",
    "    tmp[0] = (0, 0, 0) \n",
    "    cmap = mcolors.ListedColormap(tmp)\n",
    "    \n",
    "    plt.figure(figsize=(xmax/100, ymax/100))\n",
    "    plt.imshow(picture, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(ticks=np.arange(20)) ###\n",
    "    plt.show()\n",
    "    plt.imsave('clustered_cells.tiff', picture, cmap=cmap)\n",
    "\n",
    "    return picture\n",
    "\n",
    "clustered_picture = generate_clustered_picture()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
